{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-26 20:20:09.910125: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-26 20:20:10.813062: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-07-26 20:20:20.936981: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import torch, cv2\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "from utils.config import *\n",
    "from utils.common_utils import read_json_data\n",
    "from utils.dataset import CaptionInContext\n",
    "from utils.text_utils import get_text_metadata\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.dataset_utils import PadCollate1\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as pltc\n",
    "\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os.path\n",
    "import torch\n",
    "import itertools as it\n",
    "from torch.utils.data import Dataset\n",
    "from utils.dataset_utils import modify_caption_replace_entities\n",
    "from utils.common_utils import read_json_data\n",
    "from utils.config import num_boxes, DATA_DIR\n",
    "from utils.custom_transforms.data_aug import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform3 = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])])\n",
    "transform2 = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), transforms.ColorJitter(hue=.2, saturation=.2), transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])])\n",
    "transform1 = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), transforms.RandomHorizontalFlip(p=0.5), transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_images(imgPath, bboxes):\n",
    "    images = []\n",
    "    image = Image.open(imgPath)\n",
    "    for j, bbox in enumerate(bboxes):\n",
    "        img1 = image.crop((int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])))\n",
    "        if img1.size[0] > 0 and img1.size[1] > 0:\n",
    "            images.append(img1)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AugDataset(Dataset):\n",
    "    \"\"\"Custom dataset class for Out-of-Context Detection\"\"\"                                         ################################\n",
    "\n",
    "    def __init__(self, metadata_file, mode, transforms, text_field=None, max_samples=None, slice_start=None, slice_end=None):\n",
    "        \"\"\"\n",
    "            Initializes the dataset object\n",
    "            Args:\n",
    "                metadata_file (string): Path to the json file with annotations.\n",
    "                mode (string): train, val, test.\n",
    "                transforms (callable): Transform to be applied on a sample.\n",
    "                text_field (torchtext.Field): Vocab object for applying text transformations (used for Glove and Fasttext embeddings only)\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "        self.data = read_json_data(metadata_file)\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "        self.text_field = text_field\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "        self.flip_rotate_transform = Sequence(\n",
    "            [RandomHorizontalFlip(0.8), RandomScale(0.2, diff=True), RandomRotate(10)])\n",
    "\n",
    "        if max_samples is not None:\n",
    "            self.data = self.data[:max_samples]\n",
    "        #########################################\n",
    "        if slice_start is not None or slice_end is not None:\n",
    "        ###########################################\n",
    "            self.data = self.data[slice_start:slice_end]    \n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "            Returns sample corresponding to the index `index`\n",
    "        \"\"\"\n",
    "        img_data = self.data[index]\n",
    "        img_path = os.path.join(DATA_DIR, img_data['img_local_path'])\n",
    "        bboxes = img_data['maskrcnn_bboxes'][:10]\n",
    "\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        try:\n",
    "            img_aug, bboxes_aug = self.flip_rotate_transform(img, np.array(bboxes))\n",
    "            bboxes_aug = bboxes_aug.tolist()\n",
    "            bboxes = list(it.islice(it.cycle(bboxes_aug), num_boxes - 1))\n",
    "            img = img_aug\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            idx1 = random.randint(0, 1)\n",
    "            cap_key1 = 'caption1' if idx1 == 0 else 'caption2'\n",
    "            caption1 = img_data[cap_key1]\n",
    "            caption1 = modify_caption_replace_entities(caption1)\n",
    "\n",
    "            while True:\n",
    "                idx2 = random.randint(0, 1)\n",
    "                cap_key2 = 'caption1' if idx2 == 0 else 'caption2'\n",
    "                tgt_index = random.randint(0, len(self.data) - 1)\n",
    "                caption2 = self.data[tgt_index][cap_key2]\n",
    "                caption2 = modify_caption_replace_entities(caption2)\n",
    "                if caption1 != caption2:\n",
    "                    break\n",
    "        else:\n",
    "            src_captions = img_data['articles']\n",
    "            caption1 = src_captions[random.randint(0, len(src_captions) - 1)]['caption_modified']\n",
    "\n",
    "            while True:\n",
    "                tgt_index = random.randint(0, len(self.data) - 1)\n",
    "                tgt_captions = self.data[tgt_index]['articles']\n",
    "                caption2 = tgt_captions[random.randint(0, len(tgt_captions) - 1)]['caption_modified']\n",
    "                if caption1 != caption2:\n",
    "                    break\n",
    "        text = caption1\n",
    "        \n",
    "        return text, bboxes, img_path\n",
    "        # return img_path, img_aug1, img_aug2, images[0], images[1], images[2], images[3], images[4], images[5], images[6], images[7], images[8], images[9], bboxes, text\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            Returns length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embeddings\n",
    "text_field, word_embeddings, vocab_size = get_text_metadata()\n",
    "train_dataset = AugDataset(metadata_file=os.path.join(DATA_DIR, 'annotations', 'train_data.json'),\n",
    "                                 transforms=transform3, mode='train', text_field=text_field, max_samples=5000)\n",
    "\n",
    "val_dataset  = AugDataset(metadata_file=os.path.join(DATA_DIR, 'annotations', 'val_data.json'),\n",
    "                               transforms=transform3, mode='val', text_field=text_field, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=6, shuffle=True, collate_fn=PadCollate1())\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=6, shuffle=False, collate_fn=PadCollate1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k', device=device, jit=False)\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(CLIPModel, self).__init__()\n",
    "\n",
    "        # Preprocessing layers\n",
    "        self.model = model\n",
    "        self.fc = nn.Linear(512, 512)\n",
    "\n",
    "        # Set requires_grad to True for the parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.model.transformer.resblocks[-1].mlp.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        for param in self.model.visual.transformer.resblocks[-1].mlp.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        for param in self.model.ln_final.parameters():\n",
    "           param.requires_grad = True\n",
    "\n",
    "        for param in self.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, orgImg, img_aug1, img_aug2, images, tokenisedCaption):\n",
    "\n",
    "        # Create a list to store the encoded features for each image in the batch\n",
    "        feature_list = []\n",
    "        \n",
    "        encoded_orgImg = self.fc(self.model.encode_image(orgImg.to(device)))\n",
    "        encoded_orgImg = encoded_orgImg / encoded_orgImg.norm(dim=-1, keepdim=True)\n",
    "        feature_list.append(encoded_orgImg)\n",
    "\n",
    "        encoded_img_aug1 = self.fc(self.model.encode_image(img_aug1.to(device)))\n",
    "        encoded_img_aug1 = encoded_img_aug1 / encoded_img_aug1.norm(dim=-1, keepdim=True)\n",
    "        feature_list.append(encoded_img_aug1)\n",
    "\n",
    "        encoded_img_aug2 = self.fc(self.model.encode_image(img_aug2.to(device)))\n",
    "        encoded_img_aug2 = encoded_img_aug2 / encoded_img_aug2.norm(dim=-1, keepdim=True)\n",
    "        feature_list.append(encoded_img_aug2)\n",
    "\n",
    "        for image in images:\n",
    "            encoded_image = self.fc(self.model.encode_image(image.to(device)))\n",
    "            encoded_image = encoded_image / encoded_image.norm(dim=-1, keepdim=True)\n",
    "            feature_list.append(encoded_image)\n",
    "\n",
    "        encoded_text = self.fc(self.model.encode_text(tokenisedCaption.to(device)))\n",
    "        encoded_text = encoded_text / encoded_text.norm(dim=-1, keepdim=True)\n",
    "        feature_list.append(encoded_text)\n",
    "\n",
    "        return feature_list\n",
    "\n",
    "clipModel = CLIPModel(model)\n",
    "model_name = \"ClipModelResUnfreezeSCL_5k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        # print(self.temperature)\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "        # print(self.base_temperature)\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        # device = (torch.device('cuda:1')\n",
    "        #           if features.is_cuda\n",
    "        #           else torch.device('cuda:0'))\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        # print(logits_max)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        # print(mean_log_prob_pos)\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params 7085824\n",
      "Img Model 6823168\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam([p for p in clipModel.parameters() if p.requires_grad],lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5, verbose=True)\n",
    "\n",
    "print(\"Total Params\", sum(p.numel() for p in clipModel.parameters() if p.requires_grad))\n",
    "print(\"Img Model\", sum(p.numel() for p in clipModel.model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0.05\n",
    "criterion = SupConLoss(temperature=temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch):\n",
    "\n",
    "    train_loss = 0.\n",
    "    clipModel.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    clipModel.train()\n",
    "    clipModel.zero_grad()\n",
    "\n",
    "    # Training loop\n",
    "    for batch_idx, (caption, bboxes, img_path) in enumerate(tqdm(train_loader)):\n",
    "        batch = len(bboxes)\n",
    "        encoded_images_lists = [[] for _ in range(14)]\n",
    "        with torch.set_grad_enabled(True):      \n",
    "            for i, imgPath in enumerate(img_path):\n",
    "                tokenisedCaption = tokenizer([caption[i][:77]])\n",
    "                \n",
    "                imgPath = os.path.join(DATA_DIR, imgPath)\n",
    "                img = Image.open(imgPath)\n",
    "\n",
    "                img_aug1 = transform1(preprocess(img)).unsqueeze(0)\n",
    "                img_aug2 = transform2(preprocess(img)).unsqueeze(0)\n",
    "                orgImg = transform3(preprocess(img)).unsqueeze(0)\n",
    "                images = load_images(imgPath, bboxes[i])\n",
    "                images = [transform3(preprocess(imgs)).unsqueeze(0) for imgs in images]\n",
    "\n",
    "                features = clipModel(orgImg, img_aug1, img_aug2, images, tokenisedCaption)\n",
    "                for i, feature in enumerate(features):\n",
    "                    encoded_images_lists[i].append(feature)\n",
    "             \n",
    "            encoded_images_lists = [torch.stack(encoded_images_list) for encoded_images_list in encoded_images_lists]\n",
    "            encoded_images_lists = torch.stack(encoded_images_lists, dim=1)\n",
    "            loss = criterion(encoded_images_lists)\n",
    "            train_loss += float(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            clipModel.to(device)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize() \n",
    "            del encoded_images_lists\n",
    "\n",
    "            print('For Batch: {}, Total Loss: {:.4f}, Current Loss: {:.4f}'.format(int(batch_idx), train_loss / len(train_loader), loss))\n",
    "\n",
    "    print(' Train Epoch: {} Loss: {:.4f}'.format(epoch, train_loss / len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:32<21:18, 32.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 0.0000, Total Loss: 0.1641, Current Loss: 6.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [01:01<19:09, 30.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 1.0000, Total Loss: 0.3048, Current Loss: 5.6300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [01:30<18:22, 29.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 2.0000, Total Loss: 0.4387, Current Loss: 5.3543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [02:02<18:26, 30.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 3.0000, Total Loss: 0.5874, Current Loss: 5.9484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [02:32<17:38, 30.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 4.0000, Total Loss: 0.7208, Current Loss: 5.3386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [03:00<16:45, 29.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 5.0000, Total Loss: 0.8546, Current Loss: 5.3497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [03:28<16:03, 29.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 6.0000, Total Loss: 0.9882, Current Loss: 5.3455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [03:56<15:24, 28.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 7.0000, Total Loss: 1.1218, Current Loss: 5.3421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [04:29<15:28, 29.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 8.0000, Total Loss: 1.2550, Current Loss: 5.3272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [05:01<15:22, 30.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 9.0000, Total Loss: 1.3917, Current Loss: 5.4684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [05:33<15:01, 31.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 10.0000, Total Loss: 1.5251, Current Loss: 5.3380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [06:04<14:24, 30.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 11.0000, Total Loss: 1.6587, Current Loss: 5.3452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [06:36<14:05, 31.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 12.0000, Total Loss: 1.7924, Current Loss: 5.3474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [07:05<13:14, 30.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 13.0000, Total Loss: 1.9261, Current Loss: 5.3469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [07:33<12:30, 30.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 14.0000, Total Loss: 2.0598, Current Loss: 5.3484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [08:02<11:51, 29.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 15.0000, Total Loss: 2.1935, Current Loss: 5.3489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [08:34<11:39, 30.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 16.0000, Total Loss: 2.3272, Current Loss: 5.3481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [09:07<11:20, 30.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 17.0000, Total Loss: 2.4610, Current Loss: 5.3487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [09:35<10:32, 30.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 18.0000, Total Loss: 2.5946, Current Loss: 5.3472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [10:03<09:50, 29.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 19.0000, Total Loss: 2.7283, Current Loss: 5.3480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [10:31<09:14, 29.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 20.0000, Total Loss: 2.8620, Current Loss: 5.3478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [10:59<08:39, 28.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 21.0000, Total Loss: 2.9957, Current Loss: 5.3477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [11:28<08:07, 28.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 22.0000, Total Loss: 3.1294, Current Loss: 5.3460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [11:56<07:37, 28.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 23.0000, Total Loss: 3.2630, Current Loss: 5.3456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [12:25<07:08, 28.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 24.0000, Total Loss: 3.3966, Current Loss: 5.3448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [12:53<06:38, 28.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 25.0000, Total Loss: 3.5302, Current Loss: 5.3442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 27/40 [13:21<06:09, 28.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 26.0000, Total Loss: 3.6638, Current Loss: 5.3412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [13:49<05:40, 28.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 27.0000, Total Loss: 3.7972, Current Loss: 5.3355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [14:17<05:10, 28.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 28.0000, Total Loss: 3.9306, Current Loss: 5.3375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [14:46<04:42, 28.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 29.0000, Total Loss: 4.0641, Current Loss: 5.3416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [15:14<04:14, 28.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 30.0000, Total Loss: 4.1973, Current Loss: 5.3266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [15:42<03:46, 28.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 31.0000, Total Loss: 4.3304, Current Loss: 5.3250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [16:15<03:26, 29.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 32.0000, Total Loss: 4.4633, Current Loss: 5.3132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [16:43<02:55, 29.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 33.0000, Total Loss: 4.5963, Current Loss: 5.3199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [17:12<02:25, 29.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 34.0000, Total Loss: 4.7290, Current Loss: 5.3118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [17:41<01:55, 28.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 35.0000, Total Loss: 4.8615, Current Loss: 5.2989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 37/40 [18:09<01:25, 28.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 36.0000, Total Loss: 4.9939, Current Loss: 5.2963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 38/40 [18:38<00:57, 28.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 37.0000, Total Loss: 5.1260, Current Loss: 5.2826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [19:06<00:28, 28.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 38.0000, Total Loss: 5.2584, Current Loss: 5.2958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [19:08<00:00, 28.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch: 39.0000, Total Loss: 5.3403, Current Loss: 3.2746\n",
      " Train Epoch: 5 Loss: 5.3403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
